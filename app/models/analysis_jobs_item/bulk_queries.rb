# frozen_string_literal: true

class AnalysisJobsItem
  # Bulk queries for AnalysisJobsItem. These insert or update many records at once.
  # Note: none of these really change the state of the records, they just
  # schedule those records to be changed through the state machine later.
  module BulkQueries
    # Creates records for all scripts and audio recordings for a job.
    # It bypasses validation and callbacks and does this as a one shot
    # sql query.
    #
    # This query is idempotent and can be run multiple times for the same job.
    # It is an upsert statement that allows the same filter to add new recordings
    # to an existing job.
    #
    # The `BawWorkers::Jobs::Analysis::RemoteEnqueueJob` does
    # the actual enqueueing by monitoring for records created by this function
    # (more precisely records where `transition == :queue`).
    #
    # As per `AnalysisJob.filter_as_relation` the items generated by this command
    # are ones the creator of the analysis job is allowed to access.
    #
    # It is not great to bypass Rails' creation and validation logic but it is
    # the only way to create a large number of records efficiently.
    #
    # We have tests that ensure that the records are created correctly.
    #
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records created
    def batch_insert_items_for_job(analysis_job)
      check_job(analysis_job)

      # arel of ids of audio recordings to generate analysis job items for
      filter_query = analysis_job.filter_as_relation

      # because we use bind parameters we can reuse this query for each script
      upsert_query = generate_batch_upsert_query(filter_query)

      # we do one bulk insert per script, but make sure all inserts are committed
      # atomically (all or nothing).
      count = 0
      transaction do
        # iterate through each script for the job and create as many records as
        # there are audio recordings that are returned by the filter
        analysis_job.scripts.each do |script|
          binds = [
            # audio recording id comes from filter query
            script.id,           # $1: script_id
            analysis_job.id,     # $2: analysis_job_id
            Time.zone.now,       # $3: created_at
            AnalysisJobsItem::TRANSITION_QUEUE, # $4: transition
            # where status is ready
            AudioRecording::STATUS_READY # $5: status
          ]

          # we use exec_update instead of exec_insert because we want a count of
          # the number of records created which exec_insert does not return.
          result = AnalysisJobsItem.connection.exec_update(
            upsert_query.to_sql,
            "Bulk insert analysis job items for analysis job #{analysis_job.id} and script #{script.id}",
            binds
          )

          count += result
        end
      end

      logger.info("Created #{count} analysis job items for job #{analysis_job.id}")
      count
    end

    # Updates all analysis job items for a job, that are not already completed
    # (see `AnalysisJobsItem::COMPLETED_ITEM_STATUS_SYMBOLS`) to set the
    # `transition` value to `cancel`.
    # It bypasses validation and callbacks and does this as a one shot sql query.
    # The `BawWorkers::Jobs::Analysis::CancelItemsJob` does the actual cancelling
    # by processing these records one by one.
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records updated
    def batch_mark_items_to_cancel_for_job(analysis_job)
      check_job(analysis_job)

      # select all not terminal
      # update transition to cancel
      query = generate_batch_update_query(
        AnalysisJobsItem.not_completed_arel,
        { AnalysisJobsItem.arel_table[:cancel_started_at] => Arel::Nodes::BindParam.new('created_at') }
      )

      binds = [AnalysisJobsItem::TRANSITION_CANCEL, Time.zone.now, analysis_job.id]
      AnalysisJobsItem.connection.exec_update(
        query.to_sql,
        "Bulk update analysis job items to cancel for analysis job #{analysis_job.id}",
        binds
      )
    end

    # Marks all items for a job as cancelled.
    # Designed to be run after a batch deletion of jobs on the remote queue has
    # been completed.
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records updated
    def batch_mark_items_as_cancelled_for_job(analysis_job)
      AnalysisJobsItem
        .where(analysis_job_id: analysis_job.id)
        # we don't want to update anything that's already transitioned to something else
        # so only update things that are still set to transition cancel
        .where(transition: AnalysisJobsItem::TRANSITION_CANCEL)
        .update_all(
          # see `clear_transition_cancel`
          transition: nil,
          # see `set_result_cancelled`
          result: RESULT_CANCELLED,
          # see `set_finished_at`
          finished_at: Time.zone.now,
          # see `clear_queue_id`
          queue_id: nil,
          # this is the state machines's backing field
          status: AnalysisJobsItem::STATUS_FINISHED
        )
    end

    # Updates all failed, killed, or cancelled analysis job items for a job to
    #  set the transition value to queue.
    # It bypasses validation and callbacks and does this as a one shot sql query.
    # The BawWorkers::Jobs::Analysis::RemoteEnqueueJob does the actual enqueueing
    # by processing these records one by one.
    def batch_retry_items_for_job(analysis_job)
      check_job(analysis_job)

      # select all terminal
      # update transition to queue
      query = generate_batch_update_query(AnalysisJobsItem.failed_arel)
      binds = [AnalysisJobsItem::TRANSITION_RETRY, analysis_job.id]
      AnalysisJobsItem.connection.exec_update(
        query.to_sql,
        "Bulk update analysis job items to cancel for analysis job #{analysis_job.id}",
        binds
      )
    end

    private

    # Generates a bulk insert query for the given filter query.
    # Uses bind parameters to avoid sql injection and to allow the query planner
    # to reuse the query across multiple scripts.
    # The query includes a NOT EXISTS subquery to avoid consuming sequence IDs
    # for rows that already exist (which would happen with ON CONFLICT DO NOTHING).
    # The ON CONFLICT DO NOTHING clause is kept as a safety net for race conditions.
    #
    # Bind parameter order:
    #   $1: script_id (SELECT projection and NOT EXISTS WHERE clause)
    #   $2: analysis_job_id (SELECT projection and NOT EXISTS WHERE clause)
    #   $3: created_at (SELECT projection)
    #   $4: transition (SELECT projection)
    #   $5: STATUS_READY (filter WHERE clause from status_ready scope)
    # @param filter_query [ActiveRecord::Relation]
    # @return [Baw::Arel::UpsertManager]
    def generate_batch_upsert_query(filter_query)
      table = AnalysisJobsItem.arel_table
      ar_table = AudioRecording.arel_table

      # prepare a bulk insert
      manager = Baw::Arel::UpsertManager.new(AnalysisJobsItem)
      manager.insert(Arel::Nodes::InsertStatement.new(table).tap do |insert|
        # setup columns
        insert.columns.push(
          table[:audio_recording_id],
          table[:script_id],
          table[:analysis_job_id],
          table[:created_at],
          table[:transition]
        )

        # setup values query and new values
        # turn the relation into arel, and then augment the select manager with extra columns
        filter_arel = filter_query.arel
        raise 'must be a SelectManager' unless filter_arel.is_a?(Arel::SelectManager)

        # Extract bind params so their positions are explicit: $1=script_id, $2=analysis_job_id.
        # The NOT EXISTS subquery reuses these same positions via Arel.sql('$1')/'$2'
        # rather than adding new $6/$7 slots.
        script_id_param = Arel::Nodes::BindParam.new('script_id')       # $1
        analysis_job_id_param = Arel::Nodes::BindParam.new('analysis_job_id') # $2

        filter_arel.project(
          # audio recording id comes from filter query
          script_id_param,
          analysis_job_id_param,
          Arel::Nodes::BindParam.new('created_at'),
          Arel::Nodes::BindParam.new('transition')
        )

        # Add NOT EXISTS to avoid consuming sequence IDs for rows that already exist.
        # Without this, ON CONFLICT DO NOTHING still calls nextval() for every
        # candidate row, even those that will be skipped, burning through the sequence.
        # $1 and $2 are referenced by position to reuse the bind slots defined above.
        not_exists_subquery = table
          .project(Arel.sql('1'))
          .where(table[:audio_recording_id].eq(ar_table[:id]))
          .where(table[:script_id].eq(Arel.sql('$1')))
          .where(table[:analysis_job_id].eq(Arel.sql('$2')))
        filter_arel.where(not_exists_subquery.exists.not)

        insert.select = filter_arel
      end)

      # Keep ON CONFLICT DO NOTHING as a safety net for race conditions
      # (e.g. two workers amending a job simultaneously)
      manager.on_conflict(:do_nothing, nil, nil)
      manager.returning(nil)

      manager
    end

    def generate_batch_update_query(additional_constraint, other_fields = {})
      table = AnalysisJobsItem.arel_table

      # prepare a bulk update
      manager = Arel::UpdateManager.new(table)

      # setup columns
      manager.set(
        {
          table[:transition] => Arel::Nodes::BindParam.new('transition')
        }.merge(other_fields)
      )

      constraints = []
      # limit to the items in this job
      constraints << table[:analysis_job_id].eq(Arel::Nodes::BindParam.new('analysis_job_id'))
      constraints << additional_constraint if additional_constraint.present?
      manager.wheres = constraints

      manager
    end

    def check_job(analysis_job)
      raise ArgumentError, 'analysis_job must be an AnalysisJob' unless analysis_job.is_a?(AnalysisJob)
    end
  end
end
