# frozen_string_literal: true

class AnalysisJobsItem
  # Bulk queries for AnalysisJobsItem. These insert or update many records at once.
  # Note: none of these really change the state of the records, they just
  # schedule those records to be changed through the state machine later.
  module BulkQueries
    # Creates records for all scripts and audio recordings for a job.
    # It bypasses validation and callbacks and does this as a one shot
    # sql query.
    #
    # This query is idempotent and can be run multiple times for the same job.
    # It is an upsert statement that allows the same filter to add new recordings
    # to an existing job.
    #
    # The `BawWorkers::Jobs::Analysis::RemoteEnqueueJob` does
    # the actual enqueueing by monitoring for records created by this function
    # (more precisely records where `transition == :queue`).
    #
    # As per `AnalysisJob.filter_as_relation` the items generated by this command
    # are ones the creator of the analysis job is allowed to access.
    #
    # It is not great to bypass Rails' creation and validation logic but it is
    # the only way to create a large number of records efficiently.
    #
    # We have tests that ensure that the records are created correctly.
    #
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records created
    def batch_insert_items_for_job(analysis_job)
      check_job(analysis_job)

      # arel of ids of audio recordings to generate analysis job items for
      filter_query = analysis_job.filter_as_relation

      # because we use bind parameters we can reuse this query for each script
      upsert_query = generate_batch_upsert_query(filter_query)

      # we do one bulk insert per script, but make sure all inserts are committed
      # atomically (all or nothing).
      count = 0
      transaction do
        # iterate through each script for the job and create as many records as
        # there are audio recordings that are returned by the filter
        analysis_job.scripts.each do |script|
          binds = [
            # audio recording id comes from filter query
            script.id,
            analysis_job.id,
            Time.zone.now,
            AnalysisJobsItem::TRANSITION_QUEUE
          ]

          # we use exec_update instead of exec_insert because we want a count of
          # the number of records created which exec_insert does not return.
          result = AnalysisJobsItem.connection.exec_update(
            upsert_query.to_sql,
            "Bulk upsert analysis job items for analysis job #{analysis_job.id} and script #{script.id}",
            binds
          )

          count += result
        end
      end

      logger.info("Created #{count} analysis job items for job #{analysis_job.id}")
      count
    end

    # Updates all analysis job items for a job, that are not already completed
    # (see `AnalysisJobsItem::COMPLETED_ITEM_STATUS_SYMBOLS`) to set the
    # `transition` value to `cancel`.
    # It bypasses validation and callbacks and does this as a one shot sql query.
    # The `BawWorkers::Jobs::Analysis::CancelItemsJob` does the actual cancelling
    # by processing these records one by one.
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records updated
    def batch_mark_items_to_cancel_for_job(analysis_job)
      check_job(analysis_job)

      # select all not terminal
      # update transition to cancel
      query = generate_batch_update_query(
        AnalysisJobsItem.not_completed_arel,
        { AnalysisJobsItem.arel_table[:cancel_started_at] => Arel::Nodes::BindParam.new('created_at') }
      )

      binds = [AnalysisJobsItem::TRANSITION_CANCEL, Time.zone.now, analysis_job.id]
      AnalysisJobsItem.connection.exec_update(
        query.to_sql,
        "Bulk update analysis job items to cancel for analysis job #{analysis_job.id}",
        binds
      )
    end

    # Marks all items for a job as cancelled.
    # Designed to be run after a batch deletion of jobs on the remote queue has
    # been completed.
    # @param analysis_job [AnalysisJob]
    # @return [Integer] the number of records updated
    def batch_mark_items_as_cancelled_for_job(analysis_job)
      AnalysisJobsItem
        .where(analysis_job_id: analysis_job.id)
        # we don't want to update anything that's already transitioned to something else
        # so only update things that are still set to transition cancel
        .where(transition: AnalysisJobsItem::TRANSITION_CANCEL)
        .update_all(
          # see `clear_transition_cancel`
          transition: nil,
          # see `set_result_cancelled`
          result: RESULT_CANCELLED,
          # see `set_finished_at`
          finished_at: Time.zone.now,
          # see `clear_queue_id`
          queue_id: nil,
          # this is the state machines's backing field
          status: AnalysisJobsItem::STATUS_FINISHED
        )
    end

    # Updates all failed, killed, or cancelled analysis job items for a job to
    #  set the transition value to queue.
    # It bypasses validation and callbacks and does this as a one shot sql query.
    # The BawWorkers::Jobs::Analysis::RemoteEnqueueJob does the actual enqueueing
    # by processing these records one by one.
    def batch_retry_items_for_job(analysis_job)
      check_job(analysis_job)

      # select all terminal
      # update transition to queue
      query = generate_batch_update_query(AnalysisJobsItem.failed_arel)
      binds = [AnalysisJobsItem::TRANSITION_RETRY, analysis_job.id]
      AnalysisJobsItem.connection.exec_update(
        query.to_sql,
        "Bulk update analysis job items to cancel for analysis job #{analysis_job.id}",
        binds
      )
    end

    private

    # Generates a bulk upsert query for the given filter query.
    # Uses bind parameters to avoid sql injection but that also means this
    # query can be used more than once.
    # @param filter_query [ActiveRecord::Relation]
    # @return [Baw::Arel::UpsertManager]
    def generate_batch_upsert_query(filter_query)
      table = AnalysisJobsItem.arel_table

      # prepare a bulk insert
      manager = Baw::Arel::UpsertManager.new(AnalysisJobsItem)
      manager.insert(Arel::Nodes::InsertStatement.new(table).tap do |insert|
        # setup columns
        insert.columns.push(
          table[:audio_recording_id],
          table[:script_id],
          table[:analysis_job_id],
          table[:created_at],
          table[:transition]
        )

        # setup values query and new values
        # turn the relation into arel, and then augment the select manager with extra columns
        filter_arel = filter_query.arel
        raise 'must be a SelectManager' unless filter_arel.is_a?(Arel::SelectManager)

        filter_arel.project(
          # audio recording id comes from filter query
          Arel::Nodes::BindParam.new('script_id'),
          Arel::Nodes::BindParam.new('analysis_job_id'),
          Arel::Nodes::BindParam.new('created_at'),
          Arel::Nodes::BindParam.new('transition')
        )

        insert.select = filter_arel
      end)

      manager.on_conflict(:do_nothing, nil, nil)
      manager.returning(nil)

      manager
    end

    def generate_batch_update_query(additional_constraint, other_fields = {})
      table = AnalysisJobsItem.arel_table

      # prepare a bulk update
      manager = Arel::UpdateManager.new(table)

      # setup columns
      manager.set(
        {
          table[:transition] => Arel::Nodes::BindParam.new('transition')
        }.merge(other_fields)
      )

      constraints = []
      # limit to the items in this job
      constraints << table[:analysis_job_id].eq(Arel::Nodes::BindParam.new('analysis_job_id'))
      constraints << additional_constraint if additional_constraint.present?
      manager.wheres = constraints

      manager
    end

    def check_job(analysis_job)
      raise ArgumentError, 'analysis_job must be an AnalysisJob' unless analysis_job.is_a?(AnalysisJob)
    end
  end
end
